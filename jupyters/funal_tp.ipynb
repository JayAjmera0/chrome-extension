{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model, load_model\n",
    "# Input and Concatenate layers\n",
    "from tensorflow.keras.layers import Input, Concatenate\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Custom AttentionLayer definition\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        assert isinstance(inputs, list)\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "\n",
    "        # Attention mechanism\n",
    "        score = tf.matmul(decoder_out_seq, encoder_out_seq, transpose_b=True)\n",
    "        attention_weights = tf.nn.softmax(score, axis=-1)\n",
    "        context_vector = tf.matmul(attention_weights, encoder_out_seq)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        shape1 = input_shape[1][:2] + (input_shape[0][1],)\n",
    "        shape2 = input_shape[1][:2] + (input_shape[0][0],)\n",
    "        return [shape1, shape2]\n",
    "\n",
    "# Load tokenizer\n",
    "with open('y_tokenizer.pickle', 'rb') as handle:\n",
    "    y_tokenizer = pickle.load(handle)\n",
    "\n",
    "with open('X_tokenizer.pickle', 'rb') as handle:\n",
    "    X_tokenizer = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_target_word_index=y_tokenizer.index_word\n",
    "reverse_source_word_index=X_tokenizer.index_word\n",
    "target_word_index=y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inference/Validation Phase\n",
    "#Encoding the input sequence to get the feature vector\n",
    "max_text_len = 575\n",
    "embedding_dim = 256\n",
    "latent_dim = 512\n",
    "max_summary_len = 75\n",
    "X_voc = 76409 \n",
    "encoder_inputs = tf.keras.layers.Input(shape=(max_text_len,))\n",
    "enc_emb = tf.keras.layers.Embedding(X_voc, embedding_dim, trainable=True)(encoder_inputs)\n",
    "encoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "#Decoder setup\n",
    "#These tensors will hold the states of the previous time step\n",
    "y_voc = 38333\n",
    "decoder_inputs = tf.keras.layers.Input(shape=(None,))\n",
    "dec_emb_layer = tf.keras.layers.Embedding(y_voc, embedding_dim, trainable=True)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = tf.keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.2)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=[state_h, state_c])\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_hidden_state_input = Input(shape=(max_text_len,latent_dim))\n",
    "\n",
    "#Getting the embeddings of the decoder sequence\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "#Setting the initial states to the states from the previous time step for better prediction\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#Attention inference\n",
    "attn_layer = AttentionLayer()\n",
    "attn_out, attn_weights = attn_layer([encoder_outputs, decoder_outputs])\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "decoder_inf_concat = Concatenate(axis=-1, name='concat')([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "#Adding Dense softmax layer to generate proability distribution over the target vocabulary\n",
    "decoder_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(y_voc, activation='softmax'))\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "#Final Decoder model\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "    [decoder_outputs2] + [state_h2, state_c2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function defining the implementation of inference process\n",
    "def decode_sequence(input_seq):\n",
    "    #Encoding the input as state vectors\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq)\n",
    "\n",
    "    #Generating empty target sequence of length 1\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    #Populating the first word of target sequence with the start word\n",
    "    target_seq[0, 0] = target_word_index['sostok']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c])\n",
    "\n",
    "        #Sampling a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = reverse_target_word_index[sampled_token_index]\n",
    "\n",
    "        if(sampled_token!='eostok'):\n",
    "            decoded_sentence += ' '+sampled_token\n",
    "\n",
    "        #Exit condition: either hit max length or find stop word\n",
    "        if (sampled_token == 'eostok'  or len(decoded_sentence.split()) >= (max_summary_len-1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        #Updating the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        #Updating internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if((i!=0 and i!=target_word_index['sostok']) and i!=target_word_index['eostok']):\n",
    "            newString=newString+reverse_target_word_index[i]+' '\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "        if(i!=0):\n",
    "            newString=newString+reverse_source_word_index[i]+' '\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n",
      "1/1 [==============================] - 0s 118ms/step\n",
      "1/1 [==============================] - 0s 128ms/step\n",
      "1/1 [==============================] - 0s 101ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 96ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 158ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 137ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 106ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 95ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 99ms/step\n",
      "1/1 [==============================] - 0s 105ms/step\n",
      "1/1 [==============================] - 0s 98ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 113ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 75ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      " bj zion zion goalscoring printing sterling chicken bo pigments dashcam dashcam sentimental perrie ines pta programmed cilic cilic cilic sclerosis enjoy outbid braced kingdom braced kingdom slugger divisions grass divisions zhuang dzhokhar chicken november chicken pastors specialist payoffs respectful goalscoring drafted lupo malformation malformation asprilla asprilla excise dozen segway respectful respectful pecking nafeek drafted lupo injections pigments ruthless addressed etan nastase socialise conquer conquer gamez surfer nafeek misdiagnosed misdiagnosed ruthless reconstruct perrie payoffs lupo\n"
     ]
    }
   ],
   "source": [
    "content = \"breezy sweep pen president vladimir putin wrote new chapter crimea turbulent history committing region future returned russian domain sixty years prior ukraine breakaway peninsula signed away swiftly soviet leader nikita khrushchev dealing blatant land grab eastern flank anywhere near quick easy europe member union unlike crimea rushed referendum everyone say initially slapping visa restrictions asset freezes limited number little known politicians military men europe facing urgent calls widen scope measures target russian business community particular logic run russia essentially two sides coin alexei navalny one time moscow mayoral contender house arrest opposing current regime called europe leaders ban everyone vladimir putin personal banker chelsea football club owner roman abramovich keeping money loved ones abroad asset freezes visa restrictions especially palatable options eu rolled discretionary basis without requiring cumbersome legal procedures recourse fact russia cancels visas people like time look hermitage capital founder bill browder lost right entry moscow based money dare go back russia also banned adoption orphans americans retaliation us implementation anti corruption law named sergei magnitsky browder lawyer died year moscow detention center apparently beaten death yet playing money talks card europe must ready consequences action money also walks eu leaders must ready accept sanctions two way street hurt sides targeting russia peripatetic business community would one way sapping tenuous support president putin strategy might also turn silver lining awarding eu countries chance finally deal unpleasant sides patronage including money laundering corruption inflated prize assets like london property picasso paintings years europe hold fire though trade two decades post soviet rapprochement almost billion worth commerce lot put stake true trade war would hurt russia far harder would eu least former gdp comes exports bloc europe hefty reliance russian gas would hard time keeping factories going citizens warm without power east putin flexes political muscle open trade channels keep dialogue going giving sides chance change subject talk less tensely one afford cut lifeline especially europe economy rebound russia one wane\"\n",
    "content_seq = X_tokenizer.texts_to_sequences([content])\n",
    "content_seq = pad_sequences(content_seq,  maxlen=max_text_len, padding='post')\n",
    "summary = decode_sequence(content_seq)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
